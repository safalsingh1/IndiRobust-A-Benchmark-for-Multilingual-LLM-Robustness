import os
import json
import glob
import pandas as pd
import argparse
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def load_all_metrics(results_dir: str) -> pd.DataFrame:
    """
    Load all metrics JSONs from the results directory.
    Expected pattern: {task}_{perturbation}_{level}.json
    Or deep structure from run_experiment: results/experiment/model/{task}_{perturbation}_{level}.json
    """
    # Recursive search for .json files
    # We look for files that look like metrics reports.
    # Pattern: *_*.json, usually containing 'acc_clean' or 'accuracy'
    
    files = glob.glob(os.path.join(results_dir, "**", "*.json"), recursive=True)
    
    data = []
    
    for f in files:
        if "config_snapshot" in f or "robustness_report" in f:
            # We skip robustness_report summary files if we want raw metrics,
            # BUT the user asked for aggregation across tasks/languages/types.
            # actually, the robustness summaries ARE the aggregated per-task/model files usually.
            # Let's try to parse the granular `task_perturbation_level.json` files generated by `save_robustness_metrics`.
            pass

        # Try parsing as a granular robustness report
        # The `save_robustness_metrics` saves: { lang: { summary... } }
        # Filename: task_pert_level.json
        try:
            with open(f, 'r') as fd:
                content = json.load(fd)
            
            # Heuristic check: is this a robustness report?
            # It should be Dict[Language, Dict[Metric, Val]]
            first_val = next(iter(content.values())) if content else None
            if not isinstance(first_val, dict) or 'acc_clean' not in first_val:
                continue
                
            # If valid, extract metadata from filename or path.
            # run_experiment saves to: base_dir/model_name/task_pert_level.json
            
            # Attempt to infer model from parent folder
            parent_dir = os.path.basename(os.path.dirname(f))
            model_name = parent_dir if parent_dir != "metrics" else "unknown"
            
            # Filename parsing
            basename = os.path.basename(f).replace('.json', '')
            # format: {task}_{perturbation}_{level}
            # This is ambiguous if task has underscores.
            # We can rely on the fact that we ran known tasks/perts? No.
            # Let's assume standard format: last element is level, second last is perturbation... risky.
            
            # Better: The JSON content itself doesn't have metadata? 
            # `save_robustness_metrics` just produced the dict.
            # We should rely on path str splitting.
            
            parts = basename.split('_')
            level = parts[-1]
            # Try to reconstruct.
            # If we used run_experiment, we have clean folder structures.
            # If generic, we might struggle.
            
            # Let's just create a row for every language entry found.
            for lang, metrics in content.items():
                row = {
                    "source_file": basename,
                    "model": model_name,
                    "language": lang,
                    "acc_clean": metrics.get('acc_clean'),
                    "acc_perturbed": metrics.get('acc_perturbed'),
                    "abs_drop": metrics.get('abs_drop_acc'),
                    "rel_drop": metrics.get('rel_drop_acc'),
                    "consistency": metrics.get('consistency')
                }
                data.append(row)
                
        except Exception as e:
            # Not a metric file or error reading
            continue
            
    return pd.DataFrame(data)

def generate_summary_tables(df: pd.DataFrame, output_dir: str):
    if df.empty:
        logger.warning("No data found to aggregate.")
        return

    # 1. Overall Summary by Model (Avg across all tasks/langs/perts)
    # This is a bit coarse but useful.
    # Group by Model
    overall = df.groupby('model')[['acc_clean', 'acc_perturbed', 'rel_drop', 'consistency']].mean()
    overall.to_csv(os.path.join(output_dir, 'summary_overall_model.csv'))
    
    # 2. Breakdown by Language (Avg across tasks/perts)
    lang_summary = df.groupby(['model', 'language'])[['acc_clean', 'acc_perturbed', 'rel_drop', 'consistency']].mean()
    lang_summary.to_csv(os.path.join(output_dir, 'summary_by_language.csv'))
    
    # 3. Breakdown by Perturbation?
    # We need to extract perturbation type from 'source_file' or improve metadata.
    # Since extraction is hard without strict naming, we rely on the source_file grouping if users named files well.
    # Or strict 'model' grouping.
    
    # Let's save the full flattened CSV
    df.to_csv(os.path.join(output_dir, 'all_results_flat.csv'), index=False)
    
    # JSON export of the flat data
    df.to_json(os.path.join(output_dir, 'all_results.json'), orient='records', indent=2)
    
    logger.info(f"Aggregated results saved to {output_dir}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--results_dir", default="results")
    parser.add_argument("--output_dir", default="analysis/aggregated")
    args = parser.parse_args()
    
    os.makedirs(args.output_dir, exist_ok=True)
    
    df = load_all_metrics(args.results_dir)
    print(f"Loaded {len(df)} data points.")
    generate_summary_tables(df, args.output_dir)
